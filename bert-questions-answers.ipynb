{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model_path = \"./\" + model_name + \".onnx\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# set the model to inference mode\n",
    "# It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, \n",
    "# to turn the model to inference mode. This is required since operators like dropout or batchnorm \n",
    "# behave differently in inference and training mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy inputs to the model. Adjust if neccessary.\n",
    "inputs = {\n",
    "        # list of numerical ids for the tokenized text\n",
    "        'input_ids':   torch.randint(32, [1, 32], dtype=torch.long), \n",
    "        # dummy list of ones\n",
    "        'attention_mask': torch.ones([1, 32], dtype=torch.long),     \n",
    "        # dummy list of ones\n",
    "        'token_type_ids':  torch.ones([1, 32], dtype=torch.long)     \n",
    "    }\n",
    "\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(model,                                         \n",
    "# model being run\n",
    "                  (inputs['input_ids'],\n",
    "                   inputs['attention_mask'], \n",
    "                   inputs['token_type_ids']),                    # model input (or a tuple for multiple inputs)\n",
    "                  model_path,                                    # where to save the model (can be a file or file-like object)\n",
    "                  opset_version=11,                              # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                      # whether to execute constant folding for optimization\n",
    "                  input_names=['input_ids',\n",
    "                               'input_mask', \n",
    "                               'segment_ids'],                   # the model's input names\n",
    "                  output_names=['start_logits', \"end_logits\"],   # the model's output names\n",
    "                  dynamic_axes={'input_ids': symbolic_names,\n",
    "                                'input_mask' : symbolic_names,\n",
    "                                'segment_ids' : symbolic_names,\n",
    "                                'start_logits' : symbolic_names, \n",
    "                                'end_logits': symbolic_names})   # variable length axes/dynamic input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# The pre process function take a question and a context, and generates the tensor inputs to the model:\n",
    "# - input_ids: the words in the question encoded as integers\n",
    "# - attention_mask: not used in this model\n",
    "# - token_type_ids: a list of 0s and 1s that distinguish between the words of the question and the words of the context\n",
    "# This function also returns the words contained in the question and the context, so that the answer can be decoded into a phrase. \n",
    "def preprocess(question, context):\n",
    "    encoded_input = tokenizer(question, context)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_input.input_ids)\n",
    "    return (encoded_input.input_ids, encoded_input.attention_mask, encoded_input.token_type_ids, tokens)\n",
    "\n",
    "# The post process function maps the list of start and end log probabilities onto a text answer, using the text tokens from the question\n",
    "# and context. \n",
    "def postprocess(tokens, start, end):\n",
    "    results = {}\n",
    "    answer_start = np.argmax(start)\n",
    "    answer_end = np.argmax(end)\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "        results['answer'] = answer.capitalize()\n",
    "    else:\n",
    "        results['error'] = \"I am unable to find the answer to this question. Can you please ask another question?\"\n",
    "    return results\n",
    "\n",
    "# Perform the one-off initialization for the prediction. The init code is run once when the endpoint is setup.\n",
    "def init():\n",
    "    global tokenizer, session, model\n",
    "\n",
    "    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "    model = transformers.BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    # use AZUREML_MODEL_DIR to get your deployed model(s). If multiple models are deployed, \n",
    "    # model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), '$MODEL_NAME/$VERSION/$MODEL_FILE_NAME')\n",
    "    model_dir = os.getenv('AZUREML_MODEL_DIR')\n",
    "    if model_dir == None:\n",
    "        model_dir = \"./\"\n",
    "    model_path = os.path.join(model_dir, model_name + \".onnx\")\n",
    "\n",
    "    # Create the tokenizer\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Create an ONNX Runtime session to run the ONNX model\n",
    "    session = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])  \n",
    "\n",
    "\n",
    "# Run the PyTorch model, for functional and performance comparison\n",
    "def run_pytorch(raw_data):\n",
    "    inputs = json.loads(raw_data)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logging.info(\"Question:\", inputs[\"question\"])\n",
    "    logging.info(\"Context: \", inputs[\"context\"])\n",
    "\n",
    "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
    "    model_outputs = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))\n",
    "    return postprocess(tokens, model_outputs.start_logits.detach().numpy(), model_outputs.end_logits.detach().numpy())\n",
    "\n",
    "# Run the ONNX model with ONNX Runtime\n",
    "def run(raw_data):\n",
    "    logging.info(\"Request received\")\n",
    "    inputs = json.loads(raw_data)\n",
    "    logging.info(inputs)\n",
    "\n",
    "    # Preprocess the question and context into tokenized ids\n",
    "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
    "    logging.info(\"Running inference\")\n",
    "    \n",
    "    # Format the inputs for ONNX Runtime\n",
    "    model_inputs = {\n",
    "        'input_ids':   [input_ids], \n",
    "        'input_mask':  [input_mask],\n",
    "        'segment_ids': [segment_ids]\n",
    "        }\n",
    "\n",
    "    print(model_inputs)          \n",
    "\n",
    "    outputs = session.run(['start_logits', 'end_logits'], model_inputs)\n",
    "    logging.info(\"Post-processing\")  \n",
    "\n",
    "    # Post process the output of the model into an answer (or an error if the question could not be answered)\n",
    "    results = postprocess(tokens, outputs[0], outputs[1])\n",
    "\n",
    "    logging.info(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"{\\\"question\\\": \\\"What is Dolly Parton's middle name?\\\", \\\"context\\\": \\\"Dolly Rebecca Parton is an American singer-songwriter\\\"}\"\n",
    "\n",
    "print(run(input))\n",
    "\n",
    "# run_pytorch(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Transformers version: \", transformers.__version__)\n",
    "torch_version = torch.__version__\n",
    "print(\"Torch (ONNX exporter) version: \", torch_version)\n",
    "print(\"ONNX Runtime version: \", onnxruntime.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
