{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "# model_path = \"./\" + model_name + \".onnx\"\n",
    "model_path = \"./bert-base-NER.onnx\"\n",
    "model = BertForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# set the model to inference mode\n",
    "# It is important to call torch_model.eval() or torch_model.train(False) before exporting the model, \n",
    "# to turn the model to inference mode. This is required since operators like dropout or batchnorm \n",
    "# behave differently in inference and training mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate dummy inputs to the model. Adjust if neccessary.\n",
    "inputs = {\n",
    "        # list of numerical ids for the tokenized text\n",
    "        'input_ids':   torch.randint(32, [1, 32], dtype=torch.long), \n",
    "        # dummy list of ones\n",
    "        'attention_mask': torch.ones([1, 32], dtype=torch.long),     \n",
    "        # dummy list of ones\n",
    "        'token_type_ids':  torch.ones([1, 32], dtype=torch.long)     \n",
    "    }\n",
    "\n",
    "symbolic_names = {0: 'batch_size', 1: 'max_seq_len'}\n",
    "torch.onnx.export(model,                                         \n",
    "# model being run\n",
    "                  (inputs['input_ids'],\n",
    "                   inputs['attention_mask'], \n",
    "                   inputs['token_type_ids']),                    # model input (or a tuple for multiple inputs)\n",
    "                  model_path,                                    # where to save the model (can be a file or file-like object)\n",
    "                  opset_version=11,                              # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                      # whether to execute constant folding for optimization\n",
    "                  input_names=['input_ids', 'attention_mask', 'token_type_ids'],                   # the model's input names\n",
    "                  output_names=['logits']   # the model's output names\n",
    "                  )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import numpy as np\n",
    "import onnxruntime\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# The pre process function take a question and a context, and generates the tensor inputs to the model:\n",
    "# - input_ids: the words in the question encoded as integers\n",
    "# - attention_mask: not used in this model\n",
    "# - token_type_ids: a list of 0s and 1s that distinguish between the words of the question and the words of the context\n",
    "# This function also returns the words contained in the question and the context, so that the answer can be decoded into a phrase. \n",
    "def preprocess(sentence):\n",
    "    encoded_input = tokenizer(sentence)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded_input.input_ids)\n",
    "    return (encoded_input.input_ids, encoded_input.attention_mask, encoded_input.token_type_ids, tokens)\n",
    "\n",
    "# # The post process function maps the list of start and end log probabilities onto a text answer, using the text tokens from the question\n",
    "# # and context. \n",
    "# def postprocess(tokens, start, end):\n",
    "#     results = {}\n",
    "#     answer_start = np.argmax(start)\n",
    "#     answer_end = np.argmax(end)\n",
    "#     if answer_end >= answer_start:\n",
    "#         answer = tokens[answer_start]\n",
    "#         for i in range(answer_start+1, answer_end+1):\n",
    "#             if tokens[i][0:2] == \"##\":\n",
    "#                 answer += tokens[i][2:]\n",
    "#             else:\n",
    "#                 answer += \" \" + tokens[i]\n",
    "#         results['answer'] = answer.capitalize()\n",
    "#     else:\n",
    "#         results['error'] = \"I am unable to find the answer to this question. Can you please ask another question?\"\n",
    "#     return results\n",
    "\n",
    "# Perform the one-off initialization for the prediction. The init code is run once when the endpoint is setup.\n",
    "def init():\n",
    "    global tokenizer, session, model\n",
    "\n",
    "    model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "    model = transformers.BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "    # use AZUREML_MODEL_DIR to get your deployed model(s). If multiple models are deployed, \n",
    "    # model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), '$MODEL_NAME/$VERSION/$MODEL_FILE_NAME')\n",
    "    model_dir = os.getenv('AZUREML_MODEL_DIR')\n",
    "    if model_dir == None:\n",
    "        model_dir = \"./\"\n",
    "    model_path = os.path.join(model_dir, model_name + \".onnx\")\n",
    "\n",
    "    # Create the tokenizer\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Create an ONNX Runtime session to run the ONNX model\n",
    "    session = onnxruntime.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])  \n",
    "\n",
    "\n",
    "# Run the PyTorch model, for functional and performance comparison\n",
    "def run_pytorch(raw_data):\n",
    "    inputs = json.loads(raw_data)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logging.info(\"Sentence:\", inputs[\"sentence\"])\n",
    "\n",
    "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"sentence\"])\n",
    "    model_outputs = model(torch.tensor([input_ids]),  token_type_ids=torch.tensor([segment_ids]))\n",
    "    # return postprocess(tokens, model_outputs.start_logits.detach().numpy(), model_outputs.end_logits.detach().numpy())\n",
    "\n",
    "# Run the ONNX model with ONNX Runtime\n",
    "def run(raw_data):\n",
    "    logging.info(\"Request received\")\n",
    "    inputs = json.loads(raw_data)\n",
    "    logging.info(inputs)\n",
    "\n",
    "    # Preprocess the question and context into tokenized ids\n",
    "    input_ids, input_mask, segment_ids, tokens = preprocess(inputs[\"question\"], inputs[\"context\"])\n",
    "    logging.info(\"Running inference\")\n",
    "    \n",
    "    # Format the inputs for ONNX Runtime\n",
    "    model_inputs = {\n",
    "        'input_ids':   [input_ids], \n",
    "        'input_mask':  [input_mask],\n",
    "        # 'segment_ids': [segment_ids]\n",
    "        }\n",
    "\n",
    "    print(model_inputs)          \n",
    "\n",
    "    outputs = session.run(['start_logits', 'end_logits'], model_inputs)\n",
    "    logging.info(\"Post-processing\")  \n",
    "\n",
    "    # Post process the output of the model into an answer (or an error if the question could not be answered)\n",
    "    results = postprocess(tokens, outputs[0], outputs[1])\n",
    "\n",
    "    logging.info(results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alber\\Desktop\\Onix\\ner.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39msentence\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mMy name is Clara Williams and I live in Berkeley, California?\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(run(\u001b[39minput\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\alber\\Desktop\\Onix\\ner.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m# Preprocess the question and context into tokenized ids\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m input_ids, input_mask, segment_ids, tokens \u001b[39m=\u001b[39m preprocess(inputs[\u001b[39m\"\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m\"\u001b[39;49m], inputs[\u001b[39m\"\u001b[39m\u001b[39mcontext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mRunning inference\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alber/Desktop/Onix/ner.ipynb#X10sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m# Format the inputs for ONNX Runtime\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "input = \"{\\\"sentence\\\": \\\"My name is Clara Williams and I live in Berkeley, California?\\\"}\"\n",
    "\n",
    "print(run(input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version:  4.33.1\n",
      "Torch (ONNX exporter) version:  2.0.1+cpu\n",
      "ONNX Runtime version:  1.15.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformers version: \", transformers.__version__)\n",
    "torch_version = torch.__version__\n",
    "print(\"Torch (ONNX exporter) version: \", torch_version)\n",
    "print(\"ONNX Runtime version: \", onnxruntime.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
